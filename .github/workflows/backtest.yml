name: Backtest SOT Model (Poisson + ZIP)

on:
  workflow_dispatch:
    inputs:
      train_zip_model:
        description: 'Train ZIP model in addition to Poisson?'
        required: true
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

jobs:
  validate_data:
    name: 0. Validate Data Quality ✅
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: pip install -r requirements.txt
        
    - name: Run Data Validation Script 🔍
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        echo "🔍 Running data validation checks..."
        python src/services/ai/utils/data_validation.py
        
    - name: Upload Validation Report 📤
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: data-validation-report
        path: |
          data_validation_report.txt
          invalid_minutes_records.csv

  load_data:
    name: 1. Load Data from Supabase
    needs: validate_data
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: pip install -r requirements.txt
        
    - name: Execute Data Loader and Save Files 💾
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        echo "Running data_loader.py to pull data from Supabase..."
        python src/services/ai/data_loader.py
        
    - name: Upload Data Artifacts 📤
      uses: actions/upload-artifact@v4
      with:
        name: raw-backtest-data
        path: |
          player_data_raw.parquet
          team_def_data_raw.parquet

  process_data:
    name: 2. Feature Engineer (O-Factors)
    needs: load_data
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Download Raw Data Artifacts 📥
      uses: actions/download-artifact@v4
      with:
        name: raw-backtest-data
        
    - name: Run Backtest Processor Script (O-Factors)
      run: |
        echo "Running backtest_processor.py to merge and calculate O-factors..."
        python src/services/ai/backtest_processor.py 
        
    - name: Upload O-Factor Data 📤
      uses: actions/upload-artifact@v4
      with:
        name: final-feature-set-o-factors
        path: final_feature_set.parquet

  analyze_and_scale:
    name: 3. P-Factors, Correlation, Scaling
    needs: process_data
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Download O-Factor Data 📥
      uses: actions/download-artifact@v4
      with:
        name: final-feature-set-o-factors
        
    - name: Run P-Factor Engineer Script
      run: |
        echo "Running player_factor_engineer.py to calculate P-Factors..."
        python src/services/ai/player_factor_engineer.py
        
    - name: Run Correlation Analysis (Full Feature Set)
      run: |
        echo "Running correlation_analysis.py..."
        python src/services/ai/correlation_analysis.py 
        
    - name: Run Feature Scaling Script (Player Filtering and Standardization)
      run: |
        echo "Running feature_scaling.py to filter and standardize data..."
        python src/services/ai/feature_scaling.py

    - name: Upload Scaled Data 📤
      uses: actions/upload-artifact@v4
      with:
        name: scaled-feature-set
        path: |
          final_feature_set_scaled.parquet
          training_stats.json

  train_poisson:
    name: 4a. Train Poisson Model (Baseline) v4.2
    needs: analyze_and_scale
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Download Scaled Data 📥
      uses: actions/download-artifact@v4
      with:
        name: scaled-feature-set

    - name: Run Backtesting Model and Save Model
      run: |
        echo "Running backtest_model.py to train Poisson model v4.2..."
        python src/services/ai/backtest_model.py

    # ✅ FIXED: Upload artifacts from the correct location
    - name: Upload Poisson Model Artifacts 📤
      uses: actions/upload-artifact@v4
      with:
        name: poisson-model-artifacts
        path: |
          src/services/ai/artifacts/poisson_model.pkl
          src/services/ai/artifacts/training_stats.json

  train_zip:
    name: 4b. Train ZIP Model v4.2 ⭐
    needs: analyze_and_scale
    if: ${{ github.event.inputs.train_zip_model == 'true' }}
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Download Scaled Data 📥
      uses: actions/download-artifact@v4
      with:
        name: scaled-feature-set

    - name: Train Zero-Inflated Poisson Model 🎯 v4.2
      run: |
        echo "Running zip_model_trainer.py v4.2 to train ZIP model..."
        python src/services/ai/zip_model_trainer.py

    # ✅ FIXED: Artifacts are already in the correct location
    - name: Upload ZIP Model Artifacts 📤
      uses: actions/upload-artifact@v4
      with:
        name: zip-model-artifacts
        path: |
          src/services/ai/artifacts/zip_model.pkl
          src/services/ai/artifacts/zip_training_stats.json
          src/services/ai/artifacts/training_stats.json

  compare_and_diagnose:
    name: 5. Compare Models & Run Diagnostics
    needs: [train_poisson, train_zip]
    if: always()
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Download Scaled Data 📥
      uses: actions/download-artifact@v4
      with:
        name: scaled-feature-set

    # ✅ FIXED: Download to correct path and ensure training_stats.json is included
    - name: Download Poisson Model 📥
      uses: actions/download-artifact@v4
      with:
        name: poisson-model-artifacts
        path: ./
      continue-on-error: true
      
    - name: Copy Poisson artifacts to expected location
      run: |
        mkdir -p src/services/ai/artifacts/
        cp -f poisson_model.pkl src/services/ai/artifacts/ 2>/dev/null || true
        cp -f training_stats.json src/services/ai/artifacts/ 2>/dev/null || true

    - name: Download ZIP Model 📥
      if: ${{ github.event.inputs.train_zip_model == 'true' }}
      uses: actions/download-artifact@v4
      with:
        name: zip-model-artifacts
        path: ./
      continue-on-error: true
      
    - name: Copy ZIP artifacts to expected location
      if: ${{ github.event.inputs.train_zip_model == 'true' }}
      run: |
        mkdir -p src/services/ai/artifacts/
        cp -f zip_model.pkl src/services/ai/artifacts/ 2>/dev/null || true
        cp -f zip_training_stats.json src/services/ai/artifacts/ 2>/dev/null || true
        cp -f training_stats.json src/services/ai/artifacts/ 2>/dev/null || true
        
        # Debug: List artifacts
        echo "=== Artifacts in current directory ==="
        ls -lh *.pkl *.json 2>/dev/null || echo "No pkl/json files found"
        echo "=== Artifacts in target directory ==="
        ls -lh src/services/ai/artifacts/ 2>/dev/null || echo "Directory not found"

    - name: Run Model Diagnostics 📊
      run: |
        echo "Running model diagnostics..."
        python src/services/ai/utils/model_diagnostics.py
      continue-on-error: true

    - name: Compare Poisson vs ZIP Models 🎯
      if: ${{ github.event.inputs.train_zip_model == 'true' }}
      run: |
        echo "Running model comparison..."
        python src/services/ai/compare_models.py
      continue-on-error: true

    - name: Upload Diagnostics Results 📤
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: model-diagnostics
        path: |
          diagnostics_report.txt
          vif_results.csv
          correlation_matrix.csv
          feature_target_correlations.csv
          model_comparison_report.txt

  test_predictions:
    name: 6. Test Prediction Service
    needs: compare_and_diagnose
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model_type: [poisson, zip]
      fail-fast: false
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Download Scaled Data 📥
      uses: actions/download-artifact@v4
      with:
        name: scaled-feature-set

    # ✅ FIXED: Download artifacts to temp location first
    - name: Download ${{ matrix.model_type }} Model Artifacts 📥
      uses: actions/download-artifact@v4
      with:
        name: ${{ matrix.model_type }}-model-artifacts
        path: ./temp_artifacts/
      continue-on-error: true
      
    # ✅ FIXED: Copy to expected location with proper structure
    - name: Setup artifacts for prediction
      run: |
        mkdir -p src/services/ai/artifacts/
        
        # Copy all files from temp_artifacts to proper location
        find ./temp_artifacts/ -name "*.pkl" -exec cp {} src/services/ai/artifacts/ \;
        find ./temp_artifacts/ -name "*.json" -exec cp {} src/services/ai/artifacts/ \;
        
        # Debug output
        echo "=== Files copied to artifacts directory ===" 
        ls -lh src/services/ai/artifacts/
        
        # Verify training_stats.json exists and has npxg_MA5
        if [ -f "src/services/ai/artifacts/training_stats.json" ]; then
          echo "=== Contents of training_stats.json ===" 
          cat src/services/ai/artifacts/training_stats.json
          
          # Check for npxg_MA5
          if grep -q "npxg_MA5" src/services/ai/artifacts/training_stats.json; then
            echo "✅ npxg_MA5 found in training_stats.json"
          else
            echo "❌ ERROR: npxg_MA5 NOT found in training_stats.json"
            echo "This means the model was trained with OLD v4.1 code!"
            exit 1
          fi
        else
          echo "❌ ERROR: training_stats.json not found!"
          exit 1
        fi

    - name: Run Prediction Service Simulation (${{ matrix.model_type }})
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        MODEL_TYPE: ${{ matrix.model_type }}
      run: |
        echo "Testing ${{ matrix.model_type }} model predictions..."
        python src/services/ai/live_predictor_zip.py
      continue-on-error: true
        
    - name: Upload Prediction Output 📤
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: predictions-${{ matrix.model_type }}
        path: gameweek_sot_recommendations.csv

  package_artifacts:
    name: 7. Package Final Artifacts for Deployment
    needs: test_predictions
    if: always()
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Download All Artifacts 📥
      uses: actions/download-artifact@v4

    - name: Create Deployment Package 📦
      run: |
        echo "Creating deployment package..."
        mkdir -p deployment_package/artifacts
        
        # Copy models (search recursively)
        find . -name "*.pkl" -exec cp {} deployment_package/artifacts/ \; 2>/dev/null || true
        find . -name "training_stats.json" -exec cp {} deployment_package/artifacts/ \; 2>/dev/null || true
        find . -name "zip_training_stats.json" -exec cp {} deployment_package/artifacts/ \; 2>/dev/null || true
        
        # Copy diagnostics
        find . -name "diagnostics_report.txt" -exec cp {} deployment_package/ \; 2>/dev/null || true
        find . -name "*_results.csv" -exec cp {} deployment_package/ \; 2>/dev/null || true
        find . -name "*_comparison*.txt" -exec cp {} deployment_package/ \; 2>/dev/null || true
        
        # Copy predictions
        find . -name "gameweek_sot_recommendations.csv" -exec cp {} deployment_package/ \; 2>/dev/null || true
        
        # Debug output
        echo "=== Deployment Package Contents ==="
        ls -lR deployment_package/
        
        # Create README
        cat > deployment_package/README.md << 'EOF'
        # Model Deployment Package v4.2
        
        ## ✅ v4.2 Updates
        
        **New Feature:** npxg_MA5_scaled (non-penalty expected goals)
        - 7 features total (was 6 in v4.1)
        - training_stats.json now has 3 entries (was 2)
        - Model performance expected to improve by 1-2% Pseudo R²
        
        ## Contents
        
        ### Models
        - `artifacts/poisson_model.pkl` - Standard Poisson model (7 features)
        - `artifacts/zip_model.pkl` - Zero-Inflated Poisson model (7 features)
        - `artifacts/training_stats.json` - Feature scaling (3 features: sot_MA5, sot_conceded_MA5, npxg_MA5)
        - `artifacts/zip_training_stats.json` - ZIP-specific statistics
        
        ### Diagnostics
        - `diagnostics_report.txt` - Model performance metrics
        - `model_comparison_report.txt` - Poisson vs ZIP comparison
        - `vif_results.csv` - Multicollinearity analysis
        
        ### Test Predictions
        - `gameweek_sot_recommendations.csv` - Sample predictions (includes npxG data)
        
        ## Verification
        
        Before deployment, verify training_stats.json contains:
        ```json
        {
          "sot_MA5": {...},
          "sot_conceded_MA5": {...},
          "npxg_MA5": {...}  // ← MUST be present for v4.2
        }
        ```
        
        ## Deployment Instructions
        
        1. Verify all 3 artifacts exist in `artifacts/` folder
        2. Upload to production: `src/services/ai/artifacts/`
        3. Set environment: `export MODEL_TYPE=zip` (or `poisson`)
        4. Test: `python src/services/ai/live_predictor_zip.py`
        
        ## Model Selection
        
        Check `model_comparison_report.txt`:
        - If AIC improved by >10 → Deploy ZIP
        - If AIC improved by <10 → Business decision
        - If AIC worsened → Keep Poisson
        EOF

    - name: Upload Deployment Package 📤
      uses: actions/upload-artifact@v4
      with:
        name: deployment-package
        path: deployment_package/

  summary_report:
    name: 8. Generate Workflow Summary
    needs: package_artifacts
    if: always()
    runs-on: ubuntu-latest
    steps:
    - name: Download Diagnostics 📥
      uses: actions/download-artifact@v4
      with:
        name: model-diagnostics
      continue-on-error: true

    - name: Create Workflow Summary 📋
      run: |
        echo "# 🎯 Backtest Workflow Summary v4.2" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ✅ v4.2 Update" >> $GITHUB_STEP_SUMMARY
        echo "- **New Feature:** npxg_MA5_scaled (non-penalty xG)" >> $GITHUB_STEP_SUMMARY
        echo "- **Total Features:** 7 (was 6)" >> $GITHUB_STEP_SUMMARY
        echo "- **Expected Improvement:** +1-2% Pseudo R²" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## Workflow Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Train ZIP Model:** ${{ github.event.inputs.train_zip_model }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Run ID:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## Artifacts Generated" >> $GITHUB_STEP_SUMMARY
        echo "✅ Data validation report" >> $GITHUB_STEP_SUMMARY
        echo "✅ Poisson model v4.2 (7 features)" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event.inputs.train_zip_model }}" == "true" ]; then
          echo "✅ ZIP model v4.2 (7 features)" >> $GITHUB_STEP_SUMMARY
          echo "✅ Model comparison report" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "✅ Model diagnostics" >> $GITHUB_STEP_SUMMARY
        echo "✅ Test predictions (with npxG)" >> $GITHUB_STEP_SUMMARY
        echo "✅ Deployment package" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## ⚠️ Important Verification" >> $GITHUB_STEP_SUMMARY
        echo "Before deploying, verify \`training_stats.json\` contains:" >> $GITHUB_STEP_SUMMARY
        echo "- sot_MA5 ✅" >> $GITHUB_STEP_SUMMARY
        echo "- sot_conceded_MA5 ✅" >> $GITHUB_STEP_SUMMARY
        echo "- npxg_MA5 ✅ **REQUIRED for v4.2**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. Download **deployment-package** artifact" >> $GITHUB_STEP_SUMMARY
        echo "2. Verify training_stats.json has 3 features" >> $GITHUB_STEP_SUMMARY
        echo "3. Review model_comparison_report.txt" >> $GITHUB_STEP_SUMMARY
        echo "4. Upload artifacts to production" >> $GITHUB_STEP_SUMMARY
        echo "5. Set MODEL_TYPE environment variable" >> $GITHUB_STEP_SUMMARY